{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi9emkTp6Avdio91VZX6g6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bzizmza/Second-Case-Study-Sentiment-Analysis/blob/main/SecondCaseStudy_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Case Study: Sentiment Analysis"
      ],
      "metadata": {
        "id": "Wqji4Q3HToWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents"
      ],
      "metadata": {
        "id": "pRd45UHdTvCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Second Case Study: Sentiment Analysis](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=Wqji4Q3HToWN)\n",
        "\n",
        ">>[Table of Contents](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=pRd45UHdTvCP)\n",
        "\n",
        ">>[Installing Library PyTorch](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=U9yNzCPvd9f0)\n",
        "\n",
        ">>[Clone IndoNLU Dataset](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=tbMdJazReK5x)\n",
        "\n",
        ">>[Import Library](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=3oNBm0rYf7BM)\n",
        "\n",
        ">>[Sentiment Analysis with Deep Learning](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=XPKODG1ShNeu)\n",
        "\n",
        ">>[Configuration and Load Pre-trained Model](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=Is3bMWkuhFCY)\n",
        "\n",
        ">>[Preparation of Sentiment Analysis Dataset](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=nShhFae1iLAP)\n",
        "\n",
        ">>[Test the Model with Example Sentences](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=rnoIcjayllv4)\n",
        "\n",
        ">>[Fine Tuning and Evaluation](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=DIWPIORBl5I-)\n",
        "\n",
        ">>[Sentiment Prediction](#folderId=1qpZ2VFZKnZjg3lfUtI161fyg-1LSChuy&updateTitle=true&scrollTo=oaBV5FLrpA5w)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "Xlh_CFSOT8UE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Library PyTorch"
      ],
      "metadata": {
        "id": "U9yNzCPvd9f0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZkW6yNYTlMU"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone IndoNLU Dataset"
      ],
      "metadata": {
        "id": "tbMdJazReK5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/indobenchmark/indonlu"
      ],
      "metadata": {
        "id": "w1jnfnrZfW0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "3oNBm0rYf7BM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "  \n",
        "from indonlu.utils.forward_fn import forward_sequence_classification\n",
        "from indonlu.utils.metrics import document_sentiment_metrics_fn\n",
        "from indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader"
      ],
      "metadata": {
        "id": "iOpQnGlDf_g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis with Deep Learning"
      ],
      "metadata": {
        "id": "XPKODG1ShNeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# common functions\n",
        "###\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "def count_param(module, trainable=False):\n",
        "    if trainable:\n",
        "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in module.parameters())\n",
        "    \n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "  \n",
        "def metrics_to_string(metric_dict):\n",
        "    string_list = []\n",
        "    for key, value in metric_dict.items():\n",
        "        string_list.append('{}:{:.2f}'.format(key, value))\n",
        "    return ' '.join(string_list)"
      ],
      "metadata": {
        "id": "tIX2hHwIg2am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "set_seed(19072021)"
      ],
      "metadata": {
        "id": "aOJknEFFg50m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration and Load Pre-trained Model"
      ],
      "metadata": {
        "id": "Is3bMWkuhFCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tokenizer and Config\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
        "  \n",
        "# Instantiate model\n",
        "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
      ],
      "metadata": {
        "id": "SE0AW4cFhbdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "XJfUbvzXhhBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_param(model)"
      ],
      "metadata": {
        "id": "whP2KCjxh-9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation of Sentiment Analysis Dataset"
      ],
      "metadata": {
        "id": "nShhFae1iLAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv'\n",
        "valid_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv'\n",
        "test_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/test_preprocess_masked_label.tsv'"
      ],
      "metadata": {
        "id": "0GSjdYx7iDA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentSentimentDataset(Dataset):\n",
        "    # Static constant variable\n",
        "    LABEL2INDEX = {'positive': 0, 'neutral': 1, 'negative': 2} # Map dari label string ke index\n",
        "    INDEX2LABEL = {0: 'positive', 1: 'neutral', 2: 'negative'} # Map dari Index ke label string\n",
        "    NUM_LABELS = 3 # Jumlah label\n",
        "    \n",
        "    def load_dataset(self, path):\n",
        "        df = pd.read_csv(path, sep='\\t', header=None) # Baca tsv file dengan pandas\n",
        "        df.columns = ['text','sentiment'] # Berikan nama pada kolom tabel\n",
        "        df['sentiment'] = df['sentiment'].apply(lambda lab: self.LABEL2INDEX[lab]) # Konversi string label ke index\n",
        "        return df\n",
        "    \n",
        "    def __init__(self, dataset_path, tokenizer, *args, **kwargs):\n",
        "        self.data = self.load_dataset(dataset_path) # Load tsv file\n",
        "  \n",
        "        # Assign tokenizer, disini kita menggunakan tokenizer subword dari HuggingFace\n",
        "        self.tokenizer = tokenizer \n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        data = self.data.loc[index,:] # Ambil data pada baris tertentu dari tabel\n",
        "        text, sentiment = data['text'], data['sentiment'] # Ambil nilai text dan sentiment\n",
        "        subwords = self.tokenizer.encode(text) # Tokenisasi text menjadi subword\n",
        "    \n",
        "    # Return numpy array dari subwords dan label\n",
        "        return np.array(subwords), np.array(sentiment), data['text']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Return panjang dari dataset"
      ],
      "metadata": {
        "id": "D0KserZ4icvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentSentimentDataLoader(DataLoader):\n",
        "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
        "        super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n",
        "        self.max_seq_len = max_seq_len # Assign batas maksimum subword\n",
        "        self.collate_fn = self._collate_fn # Assign fungsi collate_fn dengan fungsi yang kita definisikan\n",
        "        \n",
        "    def _collate_fn(self, batch):\n",
        "        batch_size = len(batch) # Ambil batch size\n",
        "        max_seq_len = max(map(lambda x: len(x[0]), batch)) # Cari panjang subword maksimal dari batch \n",
        "        max_seq_len = min(self.max_seq_len, max_seq_len) # Bandingkan dengan batas yang kita tentukan sebelumnya\n",
        "        \n",
        "    # Buat buffer untuk subword, mask, dan sentiment labels, inisialisasikan semuanya dengan 0\n",
        "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
        "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
        "        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n",
        "        \n",
        "    # Isi semua buffer\n",
        "        for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n",
        "            subwords = subwords[:max_seq_len]\n",
        "            subword_batch[i,:len(subwords)] = subwords\n",
        "            mask_batch[i,:len(subwords)] = 1\n",
        "            sentiment_batch[i,0] = sentiment\n",
        "            \n",
        "    # Return subword, mask, dan sentiment data\n",
        "        return subword_batch, mask_batch, sentiment_batch"
      ],
      "metadata": {
        "id": "hSQczJQplLzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
        "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
        "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
        "  \n",
        "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)  \n",
        "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)  \n",
        "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)"
      ],
      "metadata": {
        "id": "h-4X96LOlTM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "KHHH01zhlXfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
        "print(w2i)\n",
        "print(i2w)"
      ],
      "metadata": {
        "id": "ZWK1hjbmldJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Model with Example Sentences"
      ],
      "metadata": {
        "id": "rnoIcjayllv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "  \n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "  \n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "metadata": {
        "id": "P5SbIre7ln4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning and Evaluation"
      ],
      "metadata": {
        "id": "DIWPIORBl5I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "cQLSZmr7l7Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "  \n",
        "    total_train_loss = 0\n",
        "    list_hyp, list_label = [], []\n",
        "  \n",
        "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
        "    for i, batch_data in enumerate(train_pbar):\n",
        "        # Forward model\n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "  \n",
        "        # Update model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "  \n",
        "        tr_loss = loss.item()\n",
        "        total_train_loss = total_train_loss + tr_loss\n",
        "  \n",
        "        # Calculate metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "  \n",
        "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
        "            total_train_loss/(i+1), get_lr(optimizer)))\n",
        "  \n",
        "    # Calculate train metric\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
        "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
        "  \n",
        "    # Evaluate on validation\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    \n",
        "    total_loss, total_correct, total_labels = 0, 0, 0\n",
        "    list_hyp, list_label = [], []\n",
        "  \n",
        "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
        "    for i, batch_data in enumerate(pbar):\n",
        "        batch_seq = batch_data[-1]        \n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "        \n",
        "        # Calculate total loss\n",
        "        valid_loss = loss.item()\n",
        "        total_loss = total_loss + valid_loss\n",
        "  \n",
        "        # Calculate evaluation metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "  \n",
        "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
        "        \n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
        "        total_loss/(i+1), metrics_to_string(metrics)))"
      ],
      "metadata": {
        "id": "RCrES7zBmGkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "  \n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "  \n",
        "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
        "for i, batch_data in enumerate(pbar):\n",
        "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "    list_hyp += batch_hyp\n",
        "  \n",
        "# Save prediction\n",
        "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
        "df.to_csv('pred.txt', index=False)\n",
        "  \n",
        "print(df)\n",
        "# Evaluate on test\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "  \n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "  \n",
        "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
        "for i, batch_data in enumerate(pbar):\n",
        "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "    list_hyp += batch_hyp\n",
        "  \n",
        "# Save prediction\n",
        "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
        "df.to_csv('pred.txt', index=False)\n",
        "  \n",
        "print(df)"
      ],
      "metadata": {
        "id": "1WcOj0cMnm3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Prediction"
      ],
      "metadata": {
        "id": "oaBV5FLrpA5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "  \n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "  \n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "metadata": {
        "id": "Qgb_DG_9o5a4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}